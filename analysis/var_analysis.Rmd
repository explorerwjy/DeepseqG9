---
title: "deepseq"
author: "Kathryn Addabbo"
date: "December 8, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(janitor)
library(readr)
library(caret)
library(corrplot)
library(pROC) 
library(ROCR)
library(MASS)
library(randomForest)
library(expss)


benign = gzfile('benign2.tsv.gz', 'rt')
pathogenic = gzfile('pathogenic2.tsv.gz', 'rt')

path  = read.table(pathogenic, sep = '\t')
ben = read.table(benign, sep = '\t')

names(ben) <- as.matrix(ben[1,])
ben <- ben[-1,]

names(path) <- as.matrix(path[1,])
path <- path[-1,]

#merge data based on unique identifier #Chr + Pos + ref + alt
```

```{r}

b = ben %>%
  clean_names() %>%
  mutate(class = as.factor(c("B")),
         gnom_ad_exome_all = as.character(gnom_ad_exome_all)) %>%
  filter(!gnom_ad_exome_all == "0") %>%
  filter(!str_detect(gnom_ad_exome_all, "e")) 

p = path %>%
  clean_names() %>%
  mutate(class = as.factor(c("P")))


final = rbind(b, p)

final[final == "."] <- NA


final = final %>%
  filter(!chr == "Y") 

#X: NAs : eigen, integrated_fit_cons_score_rankscore                     #delete eigen and integrated columns

#Y: NAs : lrt_converted_rankscore, mutation_taster_converted_rankscores  #delete Y rows

data = final %>%
  dplyr::select(class, sift_converted_rankscore, polyphen2_hdiv_rankscore, polyphen2_hvar_rankscore, lrt_converted_rankscore, mutation_taster_converted_rankscore, mutation_assessor_score_rankscore, fathmm_converted_rankscore, provean_converted_rankscore, vest3_rankscore, meta_svm_rankscore, meta_lr_rankscore, m_cap_rankscore, cadd_phred, dann_rankscore, fathmm_mkl_coding_rankscore, geno_canyon_score_rankscore, gerp_rs_rankscore, phylo_p100way_vertebrate_rankscore, phylo_p20way_mammalian_rankscore, phast_cons100way_vertebrate_rankscore, phast_cons20way_mammalian_rankscore, si_phy_29way_log_odds_rankscore) %>%
  mutate_if(is.factor, funs(as.character)) %>%
  mutate(class = factor(class, levels = c("B", "P"), ordered = F)) %>%
  mutate_if(is.character, funs(as.numeric)) %>%
  na.omit()

summary(data)  
colSums(is.na(data))

colnames(data)
```

```{r}

corr = data %>%
  dplyr::select(-class) %>%
  na.omit()

corrplot(cor(corr), method="number",shade.col=NA, tl.col="black", tl.srt=65, tl.pos='n')
```


```{r logistic}

set.seed(64)
rowTrain <- sample(1:dim(data)[1], nrow(data)*0.80, replace = FALSE) #80%

glm.fit <- glm(class ~ . ,data = data, subset = rowTrain, family=binomial)
summary(glm.fit)

pchisq(summary(glm.fit)$deviance, summary(glm.fit)$df.residual, lower.tail = FALSE) #DOES NOT fit the data

test.pred.prob  <- predict(glm.fit, newdata = data[-rowTrain,], type="response")

test.pred <- rep("B", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] <- "P"

#B is reference, P next level therefore prediction probability is likelihood of P

# proportion of pathogenic correctly identified
sensitivity(data = as.factor(test.pred),
            reference = data$class[-rowTrain]) 

#proportion of benign correctly identified
specificity(data = as.factor(test.pred),
            reference = data$class[-rowTrain])


confusionMatrix(data = as.factor(test.pred), positive = "P",
                reference = data$class[-rowTrain]) 

reference = data$class[-rowTrain]
score = as.character(test.pred.prob)

levels = as.data.frame(reference, score) %>%
  rownames_to_column(var =  "score")

my_mean = levels %>%
  na.omit() %>%
  mutate(score = as.numeric(score)) 

mean(my_mean$score) #0.5239517
sd(my_mean$score)/2 #0.1984758

levels$prediction <- rep("B", length(test.pred.prob))
levels$prediction[levels$score >= 0.1273666 & levels$score <  0.2974388] <- "LB"
levels$prediction[levels$score >= 0.2974388 & levels$score <  0.8213905] <- "VUS"
levels$prediction[levels$score >= 0.8213905 & levels$score <  0.9205368] <- "LP"
levels$prediction[levels$score > 0.9205368] <- "P"

#losing 5785 to NAs ie 24%


#larger type II error than type I error, 
  #McNemar: reject null that they are equivalent 

roc.glm <- roc(data$class[-rowTrain], test.pred.prob)

#glm fit and predict each of them, then include all on the same plot

auc(roc.glm)

levels1 = levels %>%
  na.omit()

levels1 %>%
  mutate(score = as.numeric(score)) %>%
  ggplot(aes(score, fill = reference)) +
  geom_histogram(alpha = 0.6, position = "identity")+
  geom_vline(aes(xintercept = 0.5), colour="darkslategrey", linetype = "dashed") +
  scale_fill_manual(values=c("deepskyblue", "coral1")) +
  ggtitle("Logistic Histogram") 

levels1 %>%
  group_by(reference, prediction) %>%
  count()
```

Prediction with the logistic model results in high sensitivity and low specificity, with an area under the curve of 0.875. 

f we were to assume a bell curve of 'real' values, let's say more than 2SD below the mean = Benign, 1.5-2SD below the mean = likely benign, 1.5 below to 1.5 above =VUS, 1.5-2SD above = likely pathogenic, more than 2SD above = pathogenic

```{r logistic comparison}


#SIFT
glm.sift <- glm(class ~ sift_converted_rankscore ,data = data, subset = rowTrain, family=binomial)

sift.pred.prob  <- predict(glm.sift, newdata = data[-rowTrain,], type="response")

sift.pred <- rep("B", length(sift.pred.prob))
sift.pred[sift.pred.prob > 0.5] <- "P"

roc.sift <- roc(data$class[-rowTrain], sift.pred.prob)
auc(roc.sift)

#CADD
glm.cadd <- glm(class ~ cadd_phred, data = data, subset = rowTrain, family=binomial)


cadd.pred.prob  <- predict(glm.cadd, newdata = data[-rowTrain,], type="response")

cadd.pred <- rep("B", length(cadd.pred.prob))
cadd.pred[cadd.pred.prob > 0.5] <- "P"

cadd.glm <- roc(data$class[-rowTrain], cadd.pred.prob)

roc.cadd <- roc(data$class[-rowTrain], cadd.pred.prob)
auc(roc.cadd)

#meta_SVM
glm.msvm <- glm(class ~ meta_svm_rankscore, data = data, subset = rowTrain, family=binomial)


msvm.pred.prob  <- predict(glm.msvm, newdata = data[-rowTrain,], type="response")

msvm.pred <- rep("B", length(msvm.pred.prob))
msvm.pred[msvm.pred.prob > 0.5] <- "P"

roc.msvm <- roc(data$class[-rowTrain], msvm.pred.prob)
auc(roc.msvm)

#polyphe
glm.poly <- glm(class ~ polyphen2_hdiv_rankscore, data = data, subset = rowTrain, family=binomial)


poly.pred.prob  <- predict(glm.poly, newdata = data[-rowTrain,], type="response")

poly.pred <- rep("B", length(poly.pred.prob))
poly.pred[poly.pred.prob > 0.5] <- "P"

roc.poly <- roc(data$class[-rowTrain], poly.pred.prob)
auc(roc.poly)


ggroc(list(roc.glm, roc.cadd, roc.sift, roc.msvm, roc.poly)) +
  scale_color_manual(labels = c("DeepSeq9", "CADD", "SIFT", "Meta SVM", "Polyphen2 HDIV"),
                     values = c("red", "orange", "forestgreen", "blue", "purple")) +
  geom_abline(intercept = 1, slope = 1, color="darkslategrey", 
                 linetype="dashed", size= 0.5) +
  ggtitle("Logistic ROC Curve") 
```




```{r random forest}

set.seed(5)


data2 = data[-1,]

train <- sample(1:nrow(data2), nrow(data2)/2)
data_train = data2[train,]
data_test = data2[-train,]

#random forest

rf.var <- randomForest(class ~ ., data_train, mtry=5, importance =TRUE, na.action = na.exclude)
rf.var

importance(rf.var)

pred.test.rf <- predict(rf.var, newdata = data_test, type = "prob")

rf.prob = as.data.frame(pred.test.rf) %>%
  clean_names() %>%
  dplyr::select(-b)  %>%
  rename(probability = p) 

#using probability of benign
test_class = data_test$class
rf.prob["reference"] <- test_class


rf.prob1 = rf.prob 
rf.prob$class[rf.prob$probability > 0.5] <- "P"
rf.prob$class[rf.prob$probability <= 0.5] <- "B"

#class is reference
#pred is prediction
rf.cm = table(rf.prob$class, data_test$class)
rf.cm

mcnemar.test(rf.cm)

#accuracy
sum(diag(rf.cm))/sum(rf.cm)


#specificity = 0.872
#sensitivity = 0.918

rf.prob %>%
  ggplot(aes(probability, fill = reference)) +
  geom_histogram(alpha = 0.6, position = "identity") +
  geom_vline(aes(xintercept = 0.5), colour="darkslategrey", linetype = "dashed") +
  scale_fill_manual(values=c("deepskyblue", "coral1")) +
  ggtitle("RF Histogram") 

auc.rf = auc(data_test$class, pred.test.rf[,2])
auc.rf

roc.rf = roc(data_test$class, pred.test.rf[,2])

ggroc(roc.rf, color = "red") +
  geom_abline(intercept = 1, slope = 1, color="darkslategrey", 
                 linetype="dashed", size= 0.5) +
  ggtitle("RF ROC Curve") 

#bag.var <- randomForest(class ~ ., subset[train,], mtry=22, importance =TRUE, na.action = na.exclude)
#pred.test.bag <- predict(bag.var, newdata = subset[-train,], type = "prob")

varImpPlot(rf.var)
```


Mtry is the square root of the number of predictors available for our analysis (23) which calculates to 5. As we can see, random forest is highly accurate when predicting the larger class (in this case benign), whereas it struggles to correctly identify pathogenic variants.


f we were to assume a bell curve of 'real' values, let's say more than 2SD below the mean = Benign, 1.5-2SD below the mean = likely benign, 1.5 below to 1.5 above =VUS, 1.5-2SD above = likely pathogenic, more than 2SD above = pathogenic
```{r}

#class distinction

mean(rf.prob1$score) #0.5260489
sd(rf.prob1$score)/2 #0.198486


rf.prob1$class[rf.prob1$score < 0.1290769] <- "B"
rf.prob1$class[rf.prob1$score >= 0.1290769 & rf.prob1$score <  0.2283199] <- "LB"
rf.prob1$class[rf.prob1$score >= 0.2283199 & rf.prob1$score <  0.8237779] <- "VUS"
rf.prob1$class[rf.prob1$score >= 0.8237779 & rf.prob1$score <  0.9230209] <- "LP"
rf.prob1$class[rf.prob1$score > 0.9230209] <- "P"

cro(rf.prob1$class, rf.prob1$reference)
```
